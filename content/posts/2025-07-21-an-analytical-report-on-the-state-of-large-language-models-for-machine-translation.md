---
title: An Analytical Report on the State of Large Language Models for Machine Translation
date: 2025-07-22T00:22:00
draft: false
description: The field of machine translation (MT) is undergoing its most significant transformation since the advent of neural networks. The dominant paradigm is shifting from highly specialized, single-task Neural Machine Translation (NMT) systems to general-purpose Large Language Models (LLMs) for which high-quality translation is an emergent, rather than explicitly engineered, capability. This transition is not merely an incremental improvement but a fundamental change in the architecture, training, and application of translation technology, redefining its role in both commercial and research contexts. Understanding this shift requires an appreciation for the NMT era that preceded it and a clear-eyed analysis of the new capabilities and challenges introduced by LLMs.
---
## **The New Epoch of Machine Translation: From NMT to Generalist LLMs**

The field of machine translation (MT) is undergoing its most significant transformation since the advent of neural networks. The dominant paradigm is shifting from highly specialized, single-task Neural Machine Translation (NMT) systems to general-purpose Large Language Models (LLMs) for which high-quality translation is an emergent, rather than explicitly engineered, capability. This transition is not merely an incremental improvement but a fundamental change in the architecture, training, and application of translation technology, redefining its role in both commercial and research contexts. Understanding this shift requires an appreciation for the NMT era that preceded it and a clear-eyed analysis of the new capabilities and challenges introduced by LLMs.

### **The Legacy of Neural Machine Translation (NMT)**

For nearly a decade, Neural Machine Translation has been the undisputed state-of-the-art. The introduction of the Transformer architecture, with its novel self-attention mechanism, marked a pivotal moment, moving the field beyond the limitations of previous recurrent neural network (RNN) designs.1 The Transformer's encoder-decoder structure allowed models to weigh the importance of different words in an input sentence when producing an output, leading to dramatic improvements in fluency and accuracy.3

Google's Neural Machine Translation (GNMT) system, introduced in 2016, serves as a landmark example of this paradigm's power. By replacing the company's long-standing phrase-based statistical methods, GNMT demonstrated an average reduction in translation errors of 60% in human side-by-side evaluations.4 A key innovation of GNMT was its ability to perform "zero-shot translation"—translating between language pairs on which it had not been explicitly trained (e.g., Japanese-to-Korean after being trained on Japanese-to-English and Korean-to-English). This was achieved by learning a language-independent intermediate representation, or "interlingua," which encoded the semantics of a sentence rather than just memorizing phrase-to-phrase mappings.3

Despite these successes, the NMT paradigm carried inherent limitations that paved the way for the next wave of innovation. NMT systems were known to be computationally expensive to train and deploy, often struggled with rare or out-of-vocabulary words, and their performance on low-resource language pairs—those with insufficient training data—remained a significant challenge.2 These models were typically trained on vast but narrow datasets of parallel corpora (direct sentence-to-sentence translations), which limited their understanding of broader world knowledge and cultural context, sometimes resulting in translations that were grammatically correct but awkward or contextually inappropriate.6

### **The Paradigm Shift to Large Language Models**

The emergence of LLMs represents a departure from the specialized NMT model. Instead of being built exclusively for translation, models like OpenAI's GPT series, Google's Gemini, and Anthropic's Claude are general-purpose intelligence systems. Their powerful translation abilities are a natural consequence of being pre-trained on massive, diverse, and multilingual datasets that encompass a significant portion of the public internet, books, and code—far exceeding the scope of traditional parallel corpora.7

This fundamental difference in training data is the source of the primary advantage of LLMs in translation: a vastly superior understanding of context. By learning from trillions of tokens of text in varied formats, LLMs can grasp nuance, idiomatic expressions, cultural references, and stylistic tone to a degree that was previously unattainable.9 This allows them to produce translations that are not only accurate but also more fluent and natural-sounding.

The findings from the 2023 Conference on Machine Translation (WMT23) capture the state of this transition perfectly, with the overview paper titled "LLMs Are Here but Not Quite There Yet".12 This suggests a complex reality where LLMs are now a dominant force, often outperforming established systems in qualitative assessments, but may not yet uniformly surpass the most highly optimized, domain-specific NMT systems in every quantitative metric.

This paradigm shift has catalyzed a profound re-evaluation of how translation technology is developed and deployed. One of the most significant consequences is the repositioning of translation from a standalone product to an integrated feature. Dedicated NMT services like the original Google Translate or DeepL were built as translation *products*. For LLMs, high-quality translation is a *feature* of a much broader intelligence platform accessible via a single, versatile API. This has democratized access to elite translation capabilities, allowing any developer to embed them into their applications with minimal overhead.9 It has also forced dedicated providers to adapt. DeepL, for instance, has responded by introducing its own "next-generation" LLM-based architecture and expanding its offerings to include a writing assistant, DeepL Write, thereby competing on a wider set of AI-powered language features rather than just translation alone.14 The value proposition is evolving from "translate this text" to the more powerful and integrated "understand, analyze, and manipulate this text in any language."

Furthermore, the architecture and training of modern LLMs have effectively overcome the long-standing "pivot language" constraint. Early NMT systems, including the sophisticated GNMT, often relied on English as an intermediary for translating between two other languages.3 This process could compound errors. In contrast, massively multilingual models like Meta's NLLB-200 are explicitly designed for direct, many-to-many translation across hundreds of languages without requiring a pivot.17 This capability is a direct result of their training on vast, diverse multilingual corpora (such as the ROOTS dataset 19) rather than being limited to collections of bilingual sentence pairs. The result is a significant leap in architectural design, enabling more direct, accurate, and culturally faithful translations between a vast array of the world's languages.

## **The Vanguard of Multilingual LLMs: A Comparative Analysis**

The current landscape of translation-capable LLMs is populated by a diverse set of contenders, each with distinct architectural philosophies, training strategies, and market positions. To navigate this complex ecosystem, a detailed comparative analysis is essential. The models selected for this report represent the cutting edge from major AI labs, specialized translation providers, and the open-source community.

### **The Contenders**

The analysis focuses on a curated list of seven leading models that exemplify the key trends in the field:

* **Generalist Titans:** This category includes **GPT-4o (OpenAI)**, **Gemini 1.5 Pro (Google)**, and **Claude 3 Opus (Anthropic)**. These are state-of-the-art, closed-source, multimodal models where elite translation performance is a core feature of their broad general intelligence.  
* **Specialized Champions:** This group features **DeepL (Next-Gen)** and **Meta's NLLB-200**. These models are developed with a primary, strategic focus on translation quality and expansive language coverage, setting benchmarks for the rest of the industry.  
* **Open-Source Powerhouses:** **Mixtral 8x7B (Mistral AI)** represents the pinnacle of open-source efficiency and performance, offering a compelling alternative for custom or on-premise deployments.  
* **Incumbent Innovator:** **Microsoft Translator (ZCode Models)** showcases the evolution of a long-standing, enterprise-focused translation service into a modern, LLM-powered platform that competes at the highest levels of academic benchmarks.

### **Master Comparison Table: Translation LLMs**

The following table provides a multi-dimensional comparison of these leading models. This structure is designed to move beyond simplistic pros and cons, offering a detailed, data-driven framework for technical evaluation and strategic decision-making. It enables a direct comparison of factors ranging from raw performance and language support to architectural design and deployment cost.

| Model Name & Version | Developer/Organization | Release Date (Latest) | Core Architecture | Parameter Count (Total/Active) | Language Coverage | Key Training Datasets | Context Window (Tokens) | FLoRes-200 BLEU Score | WMT Performance Highlights | Multimodality Support | Primary Strength | API Availability | Licensing/Access Model | API Pricing Structure (per 1M units) | Key Qualitative Strengths | Notable Limitations |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **GPT-4o** | OpenAI | May 2024 20 | Transformer (Likely MoE) | \~200B (Est.) 21 | Broad, \>95 languages supported on benchmarks 22 | Proprietary mix of web data, books, licensed data | 128K 22 | Not Publicly Reported | Not submitted to recent WMT tasks | Text, Image, Audio, Video 20 | Creative Fluency & General Reasoning | Public API, Azure OpenAI Service 22 | Proprietary | \~$5 (Input) / $15 (Output) per 1M tokens (Varies) 24 | Human-like nuance, dialect understanding, strong cross-language grammar preservation 25 | Higher latency than specialized models, potential for complex hallucinations. |
| **Gemini 1.5 Pro** | Google | Feb 2024 20 | Sparse Mixture-of-Experts (MoE) Transformer 26 | \~1.5T (Est.) 27 | Broad, demonstrated on low-resource languages like Kalamang 28 | Proprietary mix of multilingual, multimodal web data | 1M (Production), up to 10M (Research) 26 | Not Publicly Reported | Not submitted to recent WMT tasks | Text, Image, Audio, Video 29 | Long-Context Translation & In-Context Learning | Public API via Google AI Studio & Vertex AI 9 | Proprietary | Text: \~$10 (Input) / $10 (Output) per 1M characters 30 | Unmatched long-document consistency, on-the-fly learning from provided materials 28 | Performance can drop on some specific vision/audio tasks vs. 1.0 Ultra.26 |
| **Claude 3 Opus** | Anthropic | Mar 2024 32 | Transformer | \~2T (Est.) 21 | Strong in major languages (Spanish, Japanese, French); tested on low-resource pairs 32 | Proprietary mix of public web data (to Aug 2023\) and licensed data 34 | 200K 32 | 21-25 BLEU (BBC Dataset, not FLoRes) 33 | Not submitted to recent WMT tasks | Text, Image 34 | Nuanced Content & Safety | Public API, Amazon Bedrock, Google Vertex AI 34 | Proprietary | $15 (Input) / $75 (Output) per 1M tokens 36 | Excellent for formal/technical docs, strong ethical guardrails, natural Korean translation 9 | Most expensive model, slower than peers, knowledge cutoff (Aug 2023).34 |
| **DeepL (Next-Gen)** | DeepL SE | 2023-2024 (Ongoing) | LLM-based; Classic is CNN+Transformer 15 | Not Disclosed | \~35 languages, strong focus on European & major Asian languages 16 | Linguee database, proprietary data 16 | Not Specified (Optimized for longer texts vs. classic) 15 | Not Publicly Reported | Not submitted to recent WMT tasks | Text, Document, Image 14 | High-Quality Nuance (Core Languages) | Public API 15 | Proprietary | \~$25 per 1M characters (incl. monthly fee) 39 | Regarded for high accuracy and natural-sounding translations, especially for DE/FR/ES 14 | Limited language coverage compared to titans, less flexible than general LLMs.16 |
| **NLLB-200** | Meta | Jul 2022 | Mixture-of-Experts (MoE) Transformer 17 | 54.5B (Largest Version) 41 | 200+ languages, including 55 African languages 17 | Custom-built parallel corpus from web data (mined with LASER3), FLORES-200 17 | Not specified, sentence-level focus | 37.5 BLEU (3.3B model) 42 | Outperforms baselines by 44% on FLORES-101; not a WMT competitor post-release | Text-only | Low-Resource Language Translation | Open-source via Hugging Face, Vertex AI 9 | Open Source (Custom License) | Free (Open Source), platform fees apply on cloud | Unrivaled performance on low-resource languages, open-source for research and custom deployment 9 | Not a general-purpose LLM; not suitable for production document translation.43 |
| **Mixtral 8x7B** | Mistral AI | Dec 2023 20 | Sparse Mixture-of-Experts (MoE) 9 | 46.7B (Total), 12.9B (Active) 21 | English, French, Italian, German, Spanish 20 | Open web data (details not specified) | 32K | Not Publicly Reported | Exceeds GPT-3.5 on most benchmarks 20 | Text-only | Open-Source Performance/Cost | Open-source via Hugging Face, platform deployments | Open Source (Apache 2.0) | Free (Open Source), platform fees apply on cloud | Best cost/performance trade-off for an open model, can be hosted on-premise for privacy 9 | Limited language support compared to others, requires MLOps expertise to deploy.20 |
| **Microsoft Translator (ZCode-DeltaLM)** | Microsoft | Nov 2021 (WMT win) | Encoder-Decoder (DeltaLM) with Multitask Learning 44 | Not Disclosed (Large Scale) | 101+ languages (WMT21 version) 44 | Proprietary mix, iterative back-translation, dual-pseudo-parallel data 44 | Not Specified | Not Publicly Reported | Won all 3 large-scale multilingual tasks at WMT21 by \>10 BLEU points 44 | Text, Speech, Document, Image 13 | Enterprise Integration & Customization | Azure AI Services API 13 | Proprietary | Standard: $10 per 1M characters; Custom: $40 per 1M characters 46 | Strong performance on academic benchmarks, deep integration with Azure, Custom Translator feature 44 | Less public focus on creative/nuanced translation compared to OpenAI/Anthropic. |

## **Architectural Underpinnings and Training Methodologies**

The remarkable capabilities of modern translation LLMs are not accidental; they are the direct result of synergistic advancements in model architecture, data processing, and training strategies. The leap in performance from one generation to the next can be attributed to three key pillars: the adoption of more efficient architectures like Mixture-of-Experts, the curation of vast and diverse multilingual datasets, and the application of sophisticated training and alignment techniques.

### **The Rise of Mixture-of-Experts (MoE): Scaling with Efficiency**

The Mixture-of-Experts (MoE) architecture has emerged as a critical innovation for building state-of-the-art LLMs at scale. Unlike traditional "dense" transformer models where every parameter is activated for every input token, an MoE model consists of numerous smaller "expert" sub-networks. For any given token, a routing mechanism dynamically selects and activates only a small fraction of these experts. This design allows the total number of parameters in a model—and thus its total knowledge capacity—to grow into the trillions, while the computational cost of inference remains manageable, as it is proportional to the size of the active experts, not the total model size.26

This architectural choice is not merely a technical detail but a strategic decision that directly enables a model's core strengths. For a model like **Gemini 1.5 Pro**, the sparse activation of experts is what makes reasoning over an unprecedented 10-million-token context window computationally feasible.26 For

**NLLB-200**, the MoE architecture is used in a particularly sophisticated manner: low-resource languages, which have limited training data and are prone to overfitting, are automatically routed to shared expert capacity. This allows them to benefit from the knowledge learned from high-resource languages, while high-resource languages can utilize specialized experts. This architectural "triage" system is fundamental to the model's success in massive multilinguality.17 In the open-source realm,

**Mixtral 8x7B** serves as a prime example of MoE's efficiency, delivering performance that surpasses the much larger dense model GPT-3.5 on many benchmarks while being significantly more cost-effective to run.9

### **The Fuel: Massive, Multilingual, and Curated Data**

The adage "data is the new oil" is especially true for LLMs. The quality, diversity, and scale of training data are the primary determinants of a model's capability. The foundation for most modern LLMs is built upon massive, web-scale corpora like **Common Crawl** and **The Pile**, which provide trillions of tokens of raw text.19

However, raw data is noisy. The leading models are distinguished by their investment in data curation. Datasets like **C4 (Colossal Clean Crawled Corpus)** and **RefinedWeb** were created by applying extensive filtering and deduplication to Common Crawl, resulting in higher-quality training material that improves model performance.19 The development of NLLB-200 provides a state-of-the-art case study in this process. Meta's team completely overhauled their data cleaning pipeline, developing a new model (LID-200) specifically to identify and filter out noise from web-scale corpora with high confidence. They also created toxicity lists for all 200 target languages to assess and remove potentially harmful content, ensuring a cleaner and safer dataset.17

For translation, parallel data remains important, but its scarcity for most language pairs necessitates strategic data augmentation. Techniques like **back-translation**, where monolingual data in a target language is translated back into the source language to create synthetic parallel sentences, are widely used by top systems from Microsoft and Meta.17 Microsoft's ZCode-DeltaLM model further employs

**dual-pseudo-parallel data generation**, where both sides of a sentence pair are synthetically generated by translating from a high-resource pivot language like English, a method crucial for covering all 10,000 language pair directions in the WMT21 challenge.44

This reveals a self-reinforcing "data flywheel" effect that creates a significant competitive advantage for well-resourced organizations. The process begins with the need to support more languages. To do this, a company like Meta must first build a better evaluation benchmark (FLORES-200) to measure progress.17 To succeed on this new benchmark, they require better and more expansive training data. This necessitates improving their data-mining tools (like the LASER toolkit) to create massive, high-quality parallel corpora.18 This improved data is then used to train a new, more capable model (NLLB-200).17 This new model, in turn, can be used to generate even better synthetic data and more accurate data-filtering tools for the

*next* generation of models. This virtuous cycle, where investment in data infrastructure (benchmarks, mining tools, curation pipelines) directly drives model capability, which then improves that very infrastructure, is a key dynamic in the race for AI supremacy.

### **The Polish: Advanced Training and Alignment Techniques**

The final piece of the puzzle lies in how the models are trained and aligned. Simply exposing a model to data is not enough. **Curriculum learning** and **progressive learning** are strategic approaches where the model is trained in stages. Microsoft's ZCode and Meta's NLLB-200 both employ this technique, typically starting with high-resource languages or cleaner subsets of data to stabilize the initial phases of training before introducing more complex, noisy, or low-resource data later on.17 This methodical approach prevents the model from being overwhelmed early in training and leads to better overall performance.

Finally, raw pre-trained models are not suitable for direct use. They must be aligned to be helpful, safe, and steerable. **Reinforcement Learning from Human Feedback (RLHF)** is the most common technique, where human evaluators rank different model responses to guide the model towards preferred behaviors. Anthropic has pioneered a variation called **Constitutional AI**, used to train its Claude models. In this process, the model is aligned with a set of explicit principles or rules (a "constitution") derived from sources like the UN Declaration of Human Rights, reducing the reliance on large-scale human labeling.34 These alignment techniques are not just about safety; they directly impact the quality of translation by refining the model's tone, reliability, and ability to follow nuanced instructions.

## **Benchmarking Performance: A Quantitative Assessment**

Evaluating the performance of translation LLMs requires a multi-faceted approach, as no single benchmark can capture the full spectrum of their capabilities. The industry relies on a combination of specialized low-resource benchmarks, prestigious academic competitions, and translated versions of general capability tests to form a comprehensive quantitative picture.

### **The Low-Resource Gauntlet: FLoRes-200**

The FLoRes-200 benchmark is the de facto industry standard for evaluating many-to-many translation quality, particularly for low- and mid-resource languages. It provides a challenging test set covering 200 languages, enabling the assessment of over 40,000 translation directions.17 Its creation was a direct response to the need for more comprehensive evaluation beyond high-resource language pairs.51

The FLoRes-200 leaderboard reveals a tight competition at the top. As of early 2024, the leading model is **GenTranslate-7B** with a BLEU score of 38.5. It is closely followed by **NLLB-3.3B** and **SeamlessM4T-Large-V1**, both scoring 37.5 BLEU.42 Meta's development of the NLLB model family was heavily driven by this benchmark. The company claims that NLLB-200 improved upon the previous state-of-the-art by an average of 44% across all directions of the older FLORES-101 benchmark, with gains exceeding 70% for some African and Indian languages, underscoring its specific design goal and strength in this area.17

### **The Academic Arena: WMT Shared Tasks**

The Conference on Machine Translation (WMT) hosts the most prestigious annual competition for the academic and research communities. Its findings provide a crucial, albeit complex, snapshot of the state-of-the-art.

Standout results from recent years demonstrate the capabilities of large, industry-backed models. At WMT21, **Microsoft's ZCode-DeltaLM** model delivered a dominant performance in the "Large-scale Multilingual Translation" track, winning all three sub-tasks by a significant margin. It scored over 10 BLEU points higher than the M2M100 baseline model on the massive 10,000 language pair evaluation, a testament to its advanced architecture and training methodology.44

However, the WMT findings also offer a more sober perspective. The detailed results from WMT22 show a fragmented landscape where different systems from various teams (including university groups like CUNI and commercial online services) win different language pairs, indicating that there is no single "best" system across the board.52 The WMT23 overview paper further highlights this nuance, concluding that while LLMs are a powerful new force, they are "not quite there yet" in all scenarios, suggesting that highly specialized and fine-tuned NMT systems can still maintain a competitive edge in these constrained tasks.12

### **Generalist Benchmarks (MMLU) in Translation**

A new trend in evaluation involves using general capability benchmarks to proxy for translation and multilingual understanding. Instead of competing directly in translation-specific tasks, major labs like OpenAI and Anthropic demonstrate their models' multilingual prowess by translating broad-based reasoning benchmarks.

OpenAI's evaluation of **GPT-4** is a prime example. The team translated the Massive Multitask Language Understanding (MMLU) benchmark—a suite of professional and academic multiple-choice questions—into 26 languages. The key finding was that GPT-4's performance in 24 of these languages *exceeded* the English-language performance of its powerful predecessor, GPT-3.5. This was true even for low-resource languages like Latvian, Welsh, and Swahili, demonstrating an exceptional ability to transfer its advanced reasoning capabilities across languages.22 Similarly, Anthropic uses multilingual versions of the MMLU and MGSM (multilingual math) benchmarks to showcase the proficiency of its

**Claude 3** models, though these tests focus more on multilingual reasoning than on the fine-grained fidelity of translation itself.33

This divergence in evaluation strategies reveals a "benchmark paradox." The official winners of academic translation competitions like WMT are often models from Meta, Microsoft, and university research teams, which are highly optimized for the specific task.42 In contrast, the commercial models from OpenAI and Anthropic, which are widely perceived by users and businesses as the state-of-the-art in practice, often abstain from these competitions. They instead prove their mettle on their own terms using generalist benchmarks.22 This creates a split between the academic SOTA and the commercial SOTA. The former is focused on constrained, task-specific performance, while the latter is optimized for broad capabilities and overall user experience. A comprehensive assessment requires considering both types of benchmarks.

Furthermore, as the top models become increasingly proficient, traditional automated metrics like BLEU are approaching a "metric ceiling," becoming less able to distinguish between high-quality translations. The top scores on FLoRes-200 are clustered within a single point of each other.42 This is why WMT has increasingly shifted its primary evaluation method to human Direct Assessment (DA), as automated metrics have been shown to correlate poorly with human judgments of quality.52 The frontier of translation quality is no longer just about grammatical accuracy, which most top models master. It has moved to more subtle, qualitative aspects like nuance, style, and cultural appropriateness. This shift necessitates the rise of more sophisticated, human-centric evaluation frameworks, moving beyond a single score to a multi-faceted assessment of a translation's fitness for purpose.

## **Qualitative Dimensions and Advanced Capabilities**

While quantitative benchmarks provide a crucial measure of performance, they often fail to capture the full extent of an LLM's translation capabilities. The true differentiation of the latest generation of models lies in their qualitative performance—their ability to handle context, nuance, and style—and their expansion into entirely new modalities beyond text.

### **Beyond Literal Translation: Context, Nuance, and Style**

One of the most significant breakthroughs enabled by LLMs is their capacity for deep contextual understanding, driven by massive context windows. Models like **Gemini 1.5 Pro**, with a production context window of 1 million tokens (and up to 10 million in research), and the **Claude 3** family, with 200,000 tokens, can process entire books, technical manuals, or legal documents in a single prompt.9 This is a game-changer for long-form translation, allowing the model to maintain consistency in terminology, character names, and pronoun references across the entire document, a task that was notoriously difficult for sentence-by-sentence NMT systems.

This deep understanding extends to the subtleties of language. User reports and qualitative analyses consistently find that models like **GPT-4o** and **Claude 3** are significantly better at translating idiomatic expressions, cultural references, and preserving grammatical features like gender across languages—a known weakness of older statistical and neural systems.6 Their training on diverse, non-parallel text gives them a "world knowledge" that allows them to produce translations that are not just literally correct but culturally and contextually appropriate.

Furthermore, the interactive nature of LLMs allows for powerful, real-time domain adaptation through prompting. An expert user can provide few-shot examples or explicit instructions within a prompt to guide the model's translation for a specialized field, such as psychoanalysis, demanding the retention of original German technical terms alongside their English translations. This on-the-fly steerability provides a level of customization that is far more agile than the static, pre-trained nature of traditional NMT systems.23

### **Zero-Shot and In-Context Learning for Low-Resource Languages**

The ability of LLMs to translate languages for which they have seen little to no direct parallel training data is one of their most remarkable feats. This "zero-shot" translation capability is an emergent property of their training on massively multilingual corpora. By processing text from hundreds of languages, these models learn a shared semantic representation space—a kind of universal interlingua—that allows them to map concepts between language pairs they have never encountered together.3

The power of this approach is taken to its logical extreme with in-context learning in long-context models. The most powerful demonstration of this is Google's experiment with **Gemini 1.5 Pro** and the Kalamang language. Kalamang is a Papuan language with fewer than 200 speakers and virtually no online presence, making it impossible to train a traditional NMT model. Researchers provided Gemini 1.5 with a 500-page grammar manual, a dictionary, and approximately 400 example sentences—all within the model's context window. From this material alone, the model was able to learn to translate from English to Kalamang with a quality comparable to a human who had studied from the same documents.28 This experiment signals a profound shift: the context window is becoming a viable alternative to traditional fine-tuning for customization and low-resource language enablement. It dramatically lowers the barrier to creating translation capabilities for the long tail of the world's languages, shifting the competitive focus from who has the best fine-tuning API to who offers the largest and most effective context window.

### **The Multimodal Frontier: Speech-to-Speech and Vision**

The capabilities of the most advanced models are no longer confined to text. The frontier is rapidly expanding to include speech and vision, moving closer to the long-held dream of a "universal translator."

**Direct Speech-to-Speech Translation (S2ST)** is a key area of innovation. Models like Meta's **SeamlessM4T v2** and Google's **Translatotron 3** are pushing beyond cascaded systems (speech-to-text followed by text-to-speech) towards end-to-end models that translate audio directly. A particularly impressive feature is the ability to preserve the original speaker's vocal style—their pitch, pace, and tone—in the translated speech, making the experience feel far more natural and personal.20 The extremely low audio latency of models like

**GPT-4o** (reportedly 232 milliseconds) makes them suitable for the kind of real-time, interactive applications that a universal translator would require.20

Simultaneously, **multimodal vision capabilities** are adding another layer of context. Models like **GPT-4o** and **Gemini 1.5** can accept images and video as part of their input.22 This enables practical use cases such as translating the text on a restaurant menu from a photo or reading a street sign in a foreign country. More fundamentally, it allows the model to use visual information to disambiguate text, a key research area in Multimodal Machine Translation (MMT) that aims to resolve ambiguity by grounding language in the visual world.55

The convergence of these distinct but related technologies—massive multilinguality, real-time S2ST, vocal style preservation, and visual understanding—indicates that the core components of a true universal translator are no longer theoretical. The challenge is shifting from fundamental research to engineering and product integration, a development that promises to have a profound impact on global communication, business, and travel.

## **Practical Considerations for Implementation and Deployment**

Beyond raw performance, the decision to adopt a specific LLM for translation hinges on a range of practical factors, including the trade-offs between open-source and proprietary models, the economics of different pricing structures, and the degree of customization and control required for a given application.

### **Open-Source vs. Proprietary APIs: The Strategic Trade-off**

The choice between using a proprietary API and deploying an open-source model represents a fundamental strategic decision with significant implications for cost, control, and data privacy.

* **Proprietary Models:** Services from OpenAI (GPT series), Google (Gemini), Anthropic (Claude), and DeepL offer state-of-the-art performance with the convenience of a fully managed API. This approach provides ease of use, eliminates the need for managing complex infrastructure, and grants access to the most powerful models available.9 However, it comes with per-transaction usage costs, potential data privacy concerns for sensitive information, and a complete lack of control over the underlying model architecture and training data.23 These services are ideal for applications where cutting-edge performance and rapid development are prioritized over cost control and data sovereignty.  
* **Open-Source Models:** Models like Mistral AI's **Mixtral 8x7B**, Meta's **Llama** series, and the specialized **NLLB-200** offer a compelling alternative. Their primary advantage is control. An open-source license (like Apache 2.0) grants the freedom to deploy the model on-premise or in a private cloud, ensuring maximum data privacy and security—a critical requirement for many enterprise use cases.1 This approach eliminates per-transaction fees, though it incurs significant infrastructure and operational costs. It also allows for deep customization and fine-tuning on proprietary data. The trade-off is the requirement for substantial in-house MLOps expertise and the computational resources needed to host and maintain these large models.17

### **The Economics of Translation: A Pricing Model Deep Dive**

The cost of using translation APIs can vary dramatically depending on the provider's pricing model. Understanding these differences is essential for calculating the total cost of ownership (TCO).

* **Per-Character Models:** This is the traditional pricing model used by dedicated NMT services. **Google Translate's** standard NMT API and **DeepL Pro** both charge based on the number of characters translated. The rate is typically around $20-$25 per million characters, often with a monthly subscription fee for the API key.30 This model is simple and highly predictable, making it easy to forecast costs. Both services also offer generous free tiers (e.g., 500,000 free characters per month), which are suitable for low-volume use and extensive testing.39  
* **Per-Token Models:** The new generation of LLMs from OpenAI, Google (Gemini), and Anthropic are priced per token (roughly equivalent to 0.75 words), with different rates for input (the prompt) and output (the generated translation). For example, a model like GPT-4o might cost approximately $5 per million input tokens and $15 per million output tokens.24 For many use cases, this can be drastically cheaper than per-character pricing; one analysis estimated a potential 800x cost reduction for a real-time translation application when using an efficient LLM compared to DeepL.59 However, this model is less predictable, as the number of tokens required for a given text can vary significantly depending on the language and the complexity of the prompt.

This shift in pricing models is leading to an inversion in the "total cost of translation." Historically, the bulk of the cost was the per-character fee paid to the MT provider. With the precipitous drop in per-token costs for powerful LLMs, the dominant cost drivers are shifting from API inference to the human expertise and infrastructure required to use the models effectively. This includes the salaries of MLOps engineers to manage open-source models, the time of linguists and prompt engineers to craft and maintain sophisticated prompts and few-shot examples 23, and the persistent cost of human post-editing (MTPE), which remains essential for high-stakes or quality-critical content.7 Consequently, selecting a model based on API price alone is a flawed strategy. A slightly more expensive API that produces higher quality output and requires less human oversight may ultimately have a lower total cost of translation.

### **Customization and Control: Glossaries, Fine-Tuning, and Prompting**

Achieving high-quality, enterprise-grade translation requires a level of customization that goes beyond the base capabilities of any model.

* **Glossaries and Terminology Management:** Ensuring the correct translation of brand names, product features, and domain-specific technical terms is non-negotiable for professional localization. Services like **DeepL** and **Microsoft's Custom Translator** provide formal support for terminology glossaries, allowing users to enforce specific translations.14  
* **Fine-Tuning:** For the highest degree of customization, fine-tuning a model on proprietary data—such as a company's entire historical translation memory (TM)—is the gold standard. This allows the model to learn the specific style, tone, and terminology of an organization. This capability is inherent to open-source models and is also offered as a managed service by cloud platforms like **Azure OpenAI** and **Google Vertex AI**.10  
* **Prompt Engineering:** The steerability of LLMs opens up powerful new avenues for real-time customization via prompting. A well-crafted prompt can instruct the model to adopt a specific tone (formal or informal), translate for a particular audience (e.g., a child or an expert), or adhere to specific formatting rules, offering a level of dynamic control that was previously impossible.23

The diverse strengths and weaknesses of the available models suggest that a "one-size-fits-all" approach is suboptimal. Research is already exploring advanced ensembling methods and multi-agent refinement workflows that combine the outputs of multiple models to achieve superior results.60 The logical trajectory for sophisticated localization providers is to move away from a single-provider strategy and towards a hybrid production pipeline. Such a system would intelligently route translation jobs to the optimal engine based on a variety of factors like language pair, content domain, quality requirements, and cost constraints. In this emerging paradigm, the orchestration layer—the software that manages this intelligent routing—becomes as strategically valuable as the underlying translation models themselves.

## **Concluding Analysis and Future Trajectory**

The landscape of machine translation has been irrevocably altered by the rise of Large Language Models. The move from specialized NMT systems to general-purpose AI has unlocked new levels of quality, fluency, and contextual understanding, while simultaneously introducing new economic models and deployment strategies. The optimal choice of model is no longer a simple question of which has the highest benchmark score, but a complex, multi-factor decision tailored to specific use cases.

### **Synthesis: Choosing the Right Model for the Job**

Based on the comprehensive analysis of their technical capabilities, performance data, and practical considerations, the following strategic recommendations can be made:

* **For Maximum Quality & Nuance (Creative/Marketing Content):** **GPT-4o** and **Claude 3 Opus** are the premier choices. Their strengths lie in generating highly fluent, human-like text that captures subtle nuances, cultural context, and creative expression, making them ideal for translating marketing copy, literature, and other content where style is paramount.25  
* **For Broadest Language Support (Low-Resource Focus):** **Meta's NLLB-200** is the undisputed, purpose-built champion. It was designed specifically to address the long tail of the world's languages and remains the go-to model for applications requiring translation for under-supported languages, particularly in the African and South Asian regions.9  
* **For Enterprise Control & On-Premise Deployment:** Open-source models like **Mixtral 8x7B** offer the best solution for organizations that prioritize data privacy, security, and the ability to deeply customize a model. Deploying on-premise or in a private cloud provides complete control over the data and the model, a critical requirement for regulated industries.9  
* **For Balanced Cost and Performance (General Enterprise Use):** **Google's Gemini 1.5 Pro** and **Anthropic's Claude 3 Sonnet** represent a strong middle ground, offering a significant portion of the capabilities of their top-tier counterparts at a more accessible price point.9  
  **DeepL** also remains a highly reliable and competitive choice, particularly for its well-regarded quality in core European and major Asian language pairs.14

### **The Road Ahead: Emerging Trends in Machine Translation**

The current pace of innovation shows no signs of slowing. Several key trends are shaping the future trajectory of the field, pointing towards a future of more capable, autonomous, and integrated translation systems.

* **Agentic Translation Workflows:** The future of translation is not a single, monolithic API call. It is an agentic process that mimics the collaborative workflow of human translators and editors. Emerging research points towards multi-agent systems where one LLM agent produces an initial translation, a second agent critiques it based on specific quality dimensions (like accuracy, fluency, or style), and a third agent synthesizes the feedback to produce a final, refined output.61  
* **Hyper-Personalization:** The combination of massive context windows and more accessible fine-tuning will enable a new level of personalization. Translation models will be adapted not just to a specific company or domain, but to the unique vocabulary, style, and preferences of an individual user, creating a truly bespoke communication experience.  
* **Responsible AI as a Core Feature:** As models become more powerful and integrated into daily life, ensuring their safety and fairness will transition from a compliance requirement to a key competitive differentiator. Proactive mitigation of biases, robust filtering of toxic content, and transparent alignment methodologies like Constitutional AI will become standard expectations for any production-grade system.17  
* **The Unification of Modalities:** The conceptual and technical barriers between text, speech, and vision translation will continue to dissolve. The ultimate goal is a single, seamless communication tool that can translate a live conversation, a complex document, or a dynamic visual scene with equal facility. The rapid progress in direct speech-to-speech translation and multimodal understanding indicates that this unified future is well within reach, promising to finally deliver on the long-held vision of a universal translator.20

#### **Works cited**

1. 10 Best Large Language Models (LLMs) of 2024: Pros, Cons, & Applications \- Revelo, accessed July 21, 2025, [https://www.revelo.com/blog/best-large-language-models](https://www.revelo.com/blog/best-large-language-models)  
2. \[2403.01985\] Transformers for Low-Resource Languages: Is Féidir Linn\! \- arXiv, accessed July 21, 2025, [https://arxiv.org/abs/2403.01985](https://arxiv.org/abs/2403.01985)  
3. Google Neural Machine Translation \- Wikipedia, accessed July 21, 2025, [https://en.wikipedia.org/wiki/Google\_Neural\_Machine\_Translation](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation)  
4. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation \- Google Research, accessed July 21, 2025, [https://research.google/pubs/googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation/](https://research.google/pubs/googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation/)  
5. \[1609.08144\] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation \- arXiv, accessed July 21, 2025, [https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144)  
6. (PDF) The Machine Translation Model \- ResearchGate, accessed July 21, 2025, [https://www.researchgate.net/publication/368500372\_The\_Machine\_Translation\_Model](https://www.researchgate.net/publication/368500372_The_Machine_Translation_Model)  
7. Evaluating Translation Quality: A Qualitative and Quantitative Assessment of Machine and LLM-Driven Arabic–English Translations \- MDPI, accessed July 21, 2025, [https://www.mdpi.com/2078-2489/16/6/440](https://www.mdpi.com/2078-2489/16/6/440)  
8. Use AI and large language models for translation \- Globalization | Microsoft Learn, accessed July 21, 2025, [https://learn.microsoft.com/en-us/globalization/localization/ai/ai-and-llms-for-translation](https://learn.microsoft.com/en-us/globalization/localization/ai/ai-and-llms-for-translation)  
9. The Best LLMs for AI Translation in 2025 \- PoliLingua.com, accessed July 21, 2025, [https://www.polilingua.com/blog/post/best-llm-ai-translation.htm](https://www.polilingua.com/blog/post/best-llm-ai-translation.htm)  
10. Evaluate large language models for your machine translation tasks on AWS, accessed July 21, 2025, [https://aws.amazon.com/blogs/machine-learning/evaluate-large-language-models-for-your-machine-translation-tasks-on-aws/](https://aws.amazon.com/blogs/machine-learning/evaluate-large-language-models-for-your-machine-translation-tasks-on-aws/)  
11. The Role of Large Language Models in Machine Translation | by David Fagbuyiro \- Medium, accessed July 21, 2025, [https://medium.com/@davidfagb/the-role-of-large-language-models-in-machine-translation-5e1f6eeeb44d](https://medium.com/@davidfagb/the-role-of-large-language-models-in-machine-translation-5e1f6eeeb44d)  
12. Findings of the 2023 Conference on Machine Translation (WMT23 ..., accessed July 21, 2025, [https://aclanthology.org/2023.wmt-1.1/](https://aclanthology.org/2023.wmt-1.1/)  
13. Microsoft Translator, accessed July 21, 2025, [https://www.microsoft.com/en-us/translator/](https://www.microsoft.com/en-us/translator/)  
14. Exploring DeepL for Machine Translation: How It Works, and How Accurate It Is \- Phrase, accessed July 21, 2025, [https://phrase.com/blog/posts/deepl/](https://phrase.com/blog/posts/deepl/)  
15. About DeepL language models – DeepL Help Center | How Can We ..., accessed July 21, 2025, [https://support.deepl.com/hc/en-us/articles/14241705319580-About-DeepL-language-models](https://support.deepl.com/hc/en-us/articles/14241705319580-About-DeepL-language-models)  
16. DeepL Translator \- Wikipedia, accessed July 21, 2025, [https://en.wikipedia.org/wiki/DeepL\_Translator](https://en.wikipedia.org/wiki/DeepL_Translator)  
17. 200 languages within a single AI model: A breakthrough in high ..., accessed July 21, 2025, [https://ai.meta.com/blog/nllb-200-high-quality-machine-translation/](https://ai.meta.com/blog/nllb-200-high-quality-machine-translation/)  
18. Meta AI Research Topic \- No Language Left Behind, accessed July 21, 2025, [https://ai.meta.com/research/no-language-left-behind/](https://ai.meta.com/research/no-language-left-behind/)  
19. Open-Sourced Training Datasets for Large Language Models (LLMs) \- Kili Technology, accessed July 21, 2025, [https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models](https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models)  
20. 10 Large Language Models That Matter to the Language Industry \- Slator, accessed July 21, 2025, [https://slator.com/10-large-language-models-that-matter-to-the-language-industry/](https://slator.com/10-large-language-models-that-matter-to-the-language-industry/)  
21. AI Model Parameter Counts: A Comprehensive Analysis \- Claude, accessed July 21, 2025, [https://claude.ai/public/artifacts/0ecdfb83-807b-4481-8456-8605d48a356c](https://claude.ai/public/artifacts/0ecdfb83-807b-4481-8456-8605d48a356c)  
22. GPT-4 | OpenAI, accessed July 21, 2025, [https://openai.com/index/gpt-4-research/](https://openai.com/index/gpt-4-research/)  
23. How to Ensure GPT-4 (Azure OpenAI) Includes Original Terms in Parentheses During Translation \- Learn Microsoft, accessed July 21, 2025, [https://learn.microsoft.com/en-us/answers/questions/2151524/how-to-ensure-gpt-4-(azure-openai)-includes-origin](https://learn.microsoft.com/en-us/answers/questions/2151524/how-to-ensure-gpt-4-\(azure-openai\)-includes-origin)  
24. Azure OpenAI Service \- Pricing, accessed July 21, 2025, [https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)  
25. Open AI "solved" translation and no one is talking about it : r/ChatGPT \- Reddit, accessed July 21, 2025, [https://www.reddit.com/r/ChatGPT/comments/1cwn4yo/open\_ai\_solved\_translation\_and\_no\_one\_is\_talking/](https://www.reddit.com/r/ChatGPT/comments/1cwn4yo/open_ai_solved_translation_and_no_one_is_talking/)  
26. Gemini 1.5: Google's Generative AI Model with Mixture of Experts Architecture \- Encord, accessed July 21, 2025, [https://encord.com/blog/google-gemini-1-5-generative-ai-model-with-mixture-of-experts/](https://encord.com/blog/google-gemini-1-5-generative-ai-model-with-mixture-of-experts/)  
27. medium.com, accessed July 21, 2025, [https://medium.com/@neltac33/gemini-1-5-pro-vs-gpt-4o-a-head-to-head-showdown-29c4cc837e7b](https://medium.com/@neltac33/gemini-1-5-pro-vs-gpt-4o-a-head-to-head-showdown-29c4cc837e7b)  
28. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context \- Kapler o AI, accessed July 21, 2025, [https://www.kapler.cz/wp-content/uploads/gemini\_v1\_5\_report.pdf](https://www.kapler.cz/wp-content/uploads/gemini_v1_5_report.pdf)  
29. Gemini 1.5 Pro | Generative AI on Vertex AI \- Google Cloud, accessed July 21, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/1-5-pro](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/1-5-pro)  
30. Cloud Translation pricing, accessed July 21, 2025, [https://cloud.google.com/translate/pricing](https://cloud.google.com/translate/pricing)  
31. Gemini 1.5 Pro \- Prompt Engineering Guide, accessed July 21, 2025, [https://www.promptingguide.ai/models/gemini-pro](https://www.promptingguide.ai/models/gemini-pro)  
32. Introducing the next generation of Claude \- Anthropic, accessed July 21, 2025, [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)  
33. Claude Translation: What the Research Reveals in 2024 \- Sunyu Transphere, accessed July 21, 2025, [https://www.transphere.com/claude-translation/](https://www.transphere.com/claude-translation/)  
34. The Claude 3 Model Family: Opus, Sonnet, Haiku \- Anthropic, accessed July 21, 2025, [https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)  
35. Anthropic's Claude in Amazon Bedrock \- AWS, accessed July 21, 2025, [https://aws.amazon.com/bedrock/anthropic/](https://aws.amazon.com/bedrock/anthropic/)  
36. Understanding Different Claude Models: A Guide to Anthropic's AI, accessed July 21, 2025, [https://teamai.com/blog/large-language-models-llms/understanding-different-claude-models/](https://teamai.com/blog/large-language-models-llms/understanding-different-claude-models/)  
37. Translating \- how good is Claude? Whats your experience? : r/ClaudeAI \- Reddit, accessed July 21, 2025, [https://www.reddit.com/r/ClaudeAI/comments/1kth69l/translating\_how\_good\_is\_claude\_whats\_your/](https://www.reddit.com/r/ClaudeAI/comments/1kth69l/translating_how_good_is_claude_whats_your/)  
38. expand human possibilities, overcome language barriers, and bring cultures closer together. \- DeepL Translate, accessed July 21, 2025, [https://www.deepl.com/files/press/companyProfile\_EN.pdf](https://www.deepl.com/files/press/companyProfile_EN.pdf)  
39. DeepL \- Immersive Translate, accessed July 21, 2025, [https://immersivetranslate.com/docs/services/deepL/](https://immersivetranslate.com/docs/services/deepL/)  
40. AI vs MT: Auto-translation comparison with examples \- SimpleLocalize, accessed July 21, 2025, [https://simplelocalize.io/blog/posts/deepl-google-translate-openai-comparison/](https://simplelocalize.io/blog/posts/deepl-google-translate-openai-comparison/)  
41. Meta Releases NLLB-200, New Open-Source Model Able To Translate 200 Languages, accessed July 21, 2025, [https://wandb.ai/telidavies/ml-news/reports/Meta-Releases-NLLB-200-New-Open-Source-Model-Able-To-Translate-200-Languages--VmlldzoyMjc4NzIz](https://wandb.ai/telidavies/ml-news/reports/Meta-Releases-NLLB-200-New-Open-Source-Model-Able-To-Translate-200-Languages--VmlldzoyMjc4NzIz)  
42. FLoRes-200 Benchmark (Machine Translation) | Papers With Code, accessed July 21, 2025, [https://paperswithcode.com/sota/machine-translation-on-flores-200](https://paperswithcode.com/sota/machine-translation-on-flores-200)  
43. NLLB – Vertex AI \- Google Cloud console, accessed July 21, 2025, [https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/nllb](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/nllb)  
44. Multilingual translation at scale: 10000 language pairs and beyond ..., accessed July 21, 2025, [https://www.microsoft.com/en-us/translator/blog/2021/11/22/multilingual-translation-at-scale-10000-language-pairs-and-beyond/](https://www.microsoft.com/en-us/translator/blog/2021/11/22/multilingual-translation-at-scale-10000-language-pairs-and-beyond/)  
45. Microsoft Translator for Adobe Experience Manager, accessed July 21, 2025, [https://www.microsoft.com/en-us/translator/business/aem/](https://www.microsoft.com/en-us/translator/business/aem/)  
46. Azure Translator Text API Pricing: Detailed Cost & Plans & Alternatives \- Spotsaas, accessed July 21, 2025, [https://www.spotsaas.com/product/azure-translator-text-api/pricing](https://www.spotsaas.com/product/azure-translator-text-api/pricing)  
47. Number of Parameters in GPT-4 (Latest Data) \- Exploding Topics, accessed July 21, 2025, [https://explodingtopics.com/blog/gpt-parameters](https://explodingtopics.com/blog/gpt-parameters)  
48. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context \- arXiv, accessed July 21, 2025, [http://arxiv.org/pdf/2403.05530](http://arxiv.org/pdf/2403.05530)  
49. Claude 3.7 Sonnet System Card | Anthropic, accessed July 21, 2025, [https://www.anthropic.com/claude-3-7-sonnet-system-card](https://www.anthropic.com/claude-3-7-sonnet-system-card)  
50. The Claude 3 Model Family: Opus, Sonnet, Haiku \- Anthropic, accessed July 21, 2025, [https://www.anthropic.com/claude-3-model-card](https://www.anthropic.com/claude-3-model-card)  
51. SEACrowd/flores200 · Datasets at Hugging Face, accessed July 21, 2025, [https://huggingface.co/datasets/SEACrowd/flores200](https://huggingface.co/datasets/SEACrowd/flores200)  
52. Findings of the 2022 Conference on Machine Translation (WMT22 ..., accessed July 21, 2025, [https://aclanthology.org/2022.wmt-1.1/](https://aclanthology.org/2022.wmt-1.1/)  
53. Proceedings of the Seventh Conference on Machine Translation (WMT) \- ACL Anthology, accessed July 21, 2025, [https://aclanthology.org/volumes/2022.wmt-1/](https://aclanthology.org/volumes/2022.wmt-1/)  
54. Zero-Shot Learning: Real-Time Language Translator | by Pradum Shukla \- Medium, accessed July 21, 2025, [https://medium.com/accredian/zero-shot-learning-real-time-language-translator-14127287d126](https://medium.com/accredian/zero-shot-learning-real-time-language-translator-14127287d126)  
55. Conference of the Association for Machine Translation in the Americas (2024), accessed July 21, 2025, [https://aclanthology.org/events/amta-2024/](https://aclanthology.org/events/amta-2024/)  
56. An Overview of Popular Machine Translation APIs' Pricing, accessed July 21, 2025, [https://www.machinetranslation.com/blog/price-comparison-of-popular-machine-translation-apis](https://www.machinetranslation.com/blog/price-comparison-of-popular-machine-translation-apis)  
57. Google Cloud Translation API Pricing 2025, accessed July 21, 2025, [https://www.g2.com/products/google-cloud-translation-api/pricing](https://www.g2.com/products/google-cloud-translation-api/pricing)  
58. API Platform | OpenAI, accessed July 21, 2025, [https://openai.com/api/](https://openai.com/api/)  
59. LLMs are 800x Cheaper for Translation than DeepL : r/LocalLLaMA \- Reddit, accessed July 21, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms\_are\_800x\_cheaper\_for\_translation\_than\_deepl/](https://www.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/)  
60. arXiv:2501.15219v1 \[cs.CL\] 25 Jan 2025, accessed July 21, 2025, [https://arxiv.org/pdf/2501.15219](https://arxiv.org/pdf/2501.15219)  
61. MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation \- arXiv, accessed July 21, 2025, [https://arxiv.org/html/2505.14848v1](https://arxiv.org/html/2505.14848v1)  
62. NLP Group has 9 papers accepted by ACL 2024, accessed July 21, 2025, [https://nlp.ict.ac.cn/en/academic\_news/202407/t20240703\_229095.html](https://nlp.ict.ac.cn/en/academic_news/202407/t20240703_229095.html)
